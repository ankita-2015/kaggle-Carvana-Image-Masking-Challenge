{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mkdir train\n\n! unzip -q ../input/carvana-image-masking-challenge/train.zip -d ./train && mv train/train train/images\n! unzip -q ../input/carvana-image-masking-challenge/train_masks.zip -d ./train && mv train/train_masks train/masks","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:41:56.471961Z","iopub.execute_input":"2022-02-23T07:41:56.472291Z","iopub.status.idle":"2022-02-23T07:42:06.801763Z","shell.execute_reply.started":"2022-02-23T07:41:56.472203Z","shell.execute_reply":"2022-02-23T07:42:06.80078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom glob import glob\nimport os\nimport numpy as np\nimport imageio\n\nl = glob('train/masks/*')\nprint(len(l))\nm = imageio.imread(l[100])\nm.shape,np.unique(m)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:55:57.601887Z","iopub.execute_input":"2022-02-23T08:55:57.602653Z","iopub.status.idle":"2022-02-23T08:55:57.60822Z","shell.execute_reply.started":"2022-02-23T08:55:57.602611Z","shell.execute_reply":"2022-02-23T08:55:57.607379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\ndef load_dataset(dataset_path):\n    images = sorted(glob(os.path.join(dataset_path, \"images/*\")))\n    masks = sorted(glob(os.path.join(dataset_path, \"masks/*\")))\n\n    train_x, test_x, train_y, test_y = train_test_split(images,masks, test_size=0.0981, \n                                                        random_state=168, shuffle=True)\n    return train_x, train_y, test_x, test_y\n\ntrain_x, train_y, val_x, val_y = load_dataset('train')\nprint(len(train_x), len(train_y), len(val_x), len(val_y))","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:55:51.301715Z","iopub.execute_input":"2022-02-23T08:55:51.302275Z","iopub.status.idle":"2022-02-23T08:55:51.306638Z","shell.execute_reply.started":"2022-02-23T08:55:51.302235Z","shell.execute_reply":"2022-02-23T08:55:51.305802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nSEED = 42\nBATCH_SIZE = 8\nBUFFER_SIZE = 2*BATCH_SIZE\n# IMG_SIZE = (1280, 1888) \n# IMG_SIZE = (640, 944)\nIMG_SIZE = (320, 480)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:56:04.081393Z","iopub.execute_input":"2022-02-23T08:56:04.081963Z","iopub.status.idle":"2022-02-23T08:56:04.088268Z","shell.execute_reply.started":"2022-02-23T08:56:04.081921Z","shell.execute_reply":"2022-02-23T08:56:04.087453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_temp = !ls train/masks/*\nimage_temp = !ls train/images/*\n\nmask_temp[0],image_temp[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:42:19.206798Z","iopub.execute_input":"2022-02-23T07:42:19.207327Z","iopub.status.idle":"2022-02-23T07:42:19.364555Z","shell.execute_reply.started":"2022-02-23T07:42:19.20729Z","shell.execute_reply":"2022-02-23T07:42:19.363716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_WIDTH, IMG_HEIGHT = 320 , 480\n\ndef parse_x_y(img_path,mask_path):\n    image = tf.io.read_file(img_path)\n    image = tf.io.decode_jpeg(image,channels=3)\n    image = tf.image.convert_image_dtype(image, tf.uint8)\n    \n    mask = tf.io.read_file(mask_path)    \n    mask = tf.io.decode_jpeg(mask,0)\n    return {'image': image, 'segmentation_mask': mask}\n\n@tf.function\ndef normalize(input_image: tf.Tensor, input_mask: tf.Tensor) -> tuple:\n    input_image = tf.cast(input_image, tf.float32) / 255.0\n    input_mask = tf.cast(input_mask, tf.float32) / 255.0\n#     input_mask = tf.cast(input_mask, tf.uint8)\n    return input_image, input_mask\n\n@tf.function\ndef load_image_train(datapoint: dict) -> tuple:\n    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE[0], IMG_SIZE[1]))\n    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE[0], IMG_SIZE[1]),method='nearest')    \n#     if tf.random.uniform(()) > 0.5:\n#         input_image = tf.image.flip_left_right(input_image)\n#         input_mask = tf.image.flip_left_right(input_mask)\n\n    input_image, input_mask = normalize(input_image, input_mask)\n    input_mask = input_mask[:,:,0]\n    input_mask = tf.expand_dims(input_mask,axis=-1)\n#     input_mask = tf.one_hot(input_mask, 1)\n#     input_mask = tf.reshape(input_mask, (IMG_SIZE[0], IMG_SIZE[1],1))\n    return input_image, input_mask\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_x,train_y))\ntrain_dataset = train_dataset.map(parse_x_y)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((val_x,val_y))\nval_dataset =val_dataset.map(parse_x_y)\n\ndataset = {\"train\": train_dataset, \"val\": val_dataset}\n\ndataset['train'] = dataset['train'].map(\n    load_image_train,\n    num_parallel_calls=tf.data.experimental.AUTOTUNE\n).shuffle(buffer_size=BUFFER_SIZE, seed=SEED).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n\ndataset['val'] = dataset['val'].map(\n    load_image_train, \n    num_parallel_calls=tf.data.experimental.AUTOTUNE\n).shuffle(buffer_size=BUFFER_SIZE, seed=SEED).cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:56:10.925999Z","iopub.execute_input":"2022-02-23T08:56:10.926965Z","iopub.status.idle":"2022-02-23T08:56:10.933119Z","shell.execute_reply.started":"2022-02-23T08:56:10.926923Z","shell.execute_reply":"2022-02-23T08:56:10.932311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X_y_paths = list(zip(train_x, train_y))\nval_X_y_paths = list(zip(val_x, val_y))","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:42:33.553797Z","iopub.execute_input":"2022-02-23T07:42:33.55427Z","iopub.status.idle":"2022-02-23T07:42:33.559158Z","shell.execute_reply.started":"2022-02-23T07:42:33.554234Z","shell.execute_reply":"2022-02-23T07:42:33.558508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image,label in dataset['train'].take(1):\n    print(\"Train image: \",image.shape)\n    print(\"Train label: \",label.shape,\"\\n\\tunique values\", np.unique(label[0]))\n    \nfor image,label in dataset['val'].take(1):\n    print(\"Val image: \",image.shape)\n    print(\"Val label: \",label.shape,\"\\n\\tunique values\", np.unique(label[0]))","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:42:40.970308Z","iopub.execute_input":"2022-02-23T07:42:40.970958Z","iopub.status.idle":"2022-02-23T07:42:43.369623Z","shell.execute_reply.started":"2022-02-23T07:42:40.970918Z","shell.execute_reply":"2022-02-23T07:42:43.367874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef display_sample(display_list):\n    \"\"\"Show side-by-side an input image,\n    the ground truth and the prediction.\n    \"\"\"\n    plt.figure(figsize=(7, 7))\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))#,cmap=\"gray\")\n        plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:42:43.371391Z","iopub.execute_input":"2022-02-23T07:42:43.371684Z","iopub.status.idle":"2022-02-23T07:42:43.377885Z","shell.execute_reply.started":"2022-02-23T07:42:43.37163Z","shell.execute_reply":"2022-02-23T07:42:43.377131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=0\nfor image, mask in dataset['train'].take(5):\n    i=i+1\n    sample_image, sample_mask = image, mask\n    display_sample([sample_image[0],sample_mask[0]])","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:42:45.155524Z","iopub.execute_input":"2022-02-23T07:42:45.155804Z","iopub.status.idle":"2022-02-23T07:42:48.696316Z","shell.execute_reply.started":"2022-02-23T07:42:45.155773Z","shell.execute_reply":"2022-02-23T07:42:48.695551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %env SM_FRAMEWORK=tf.keras\n\n!pip freeze | grep keras\n!pip freeze | grep Keras\n!pip freeze | grep tensorflow\n!pip freeze | grep h5py\n!pip freeze | grep opencv \n!pip freeze | grep pandas","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:42:48.698163Z","iopub.execute_input":"2022-02-23T07:42:48.698449Z","iopub.status.idle":"2022-02-23T07:43:05.027279Z","shell.execute_reply.started":"2022-02-23T07:42:48.698379Z","shell.execute_reply":"2022-02-23T07:43:05.026442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unet_mobilenetV2_custom","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom tensorflow.keras.layers import Conv2D, Activation, BatchNormalization\nfrom tensorflow.keras.layers import UpSampling2D, Input, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.metrics import Recall, Precision\nfrom tensorflow.keras import backend as K\n\ndef Unet_MobilenetV2(IMAGE_SIZE=IMG_SIZE, num_classes=1):\n    inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3), name=\"input_image\")\n    encoder = MobileNetV2(\n        input_tensor=inputs,\n        weights=\"imagenet\",\n        include_top=False, alpha=1.0)\n    skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n    \n#     encoder.trainable = False\n    \n    f = [16, 32, 48, 64]\n    x = encoder_output\n    for i in range(1, len(skip_connection_names)+1, 1):\n        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n        x = UpSampling2D((2, 2))(x)\n        x = Concatenate()([x, x_skip])\n        \n        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        \n        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        \n    x = Conv2D(num_classes, (1, 1), padding=\"same\")(x)\n    x = Activation(\"sigmoid\")(x)\n    \n    model = Model(inputs, x)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:43:25.236694Z","iopub.execute_input":"2022-02-23T07:43:25.237284Z","iopub.status.idle":"2022-02-23T07:43:25.25439Z","shell.execute_reply.started":"2022-02-23T07:43:25.237244Z","shell.execute_reply":"2022-02-23T07:43:25.253609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = Unet_MobilenetV2(IMAGE_SIZE=IMG_SIZE, num_classes=1)\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T03:40:38.487876Z","iopub.execute_input":"2022-02-23T03:40:38.488206Z","iopub.status.idle":"2022-02-23T03:40:38.496444Z","shell.execute_reply.started":"2022-02-23T03:40:38.488171Z","shell.execute_reply":"2022-02-23T03:40:38.495643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deeplabv3p_bonlime_official","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n\"\"\" Deeplabv3+ model for Keras.\nThis model is based on TF repo:\nhttps://github.com/tensorflow/models/tree/master/research/deeplab\nOn Pascal VOC, original model gets to 84.56% mIOU\nMobileNetv2 backbone is based on this repo:\nhttps://github.com/JonathanCMitchell/mobilenet_v2_keras\n# Reference\n- [Encoder-Decoder with Atrous Separable Convolution\n    for Semantic Image Segmentation](https://arxiv.org/pdf/1802.02611.pdf)\n- [Xception: Deep Learning with Depthwise Separable Convolutions]\n    (https://arxiv.org/abs/1610.02357)\n- [Inverted Residuals and Linear Bottlenecks: Mobile Networks for\n    Classification, Detection and Segmentation](https://arxiv.org/abs/1801.04381)\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras import layers\nfrom tensorflow.python.keras.layers import Input\nfrom tensorflow.python.keras.layers import Reshape\nfrom tensorflow.python.keras.layers import Activation\nfrom tensorflow.python.keras.layers import Concatenate\nfrom tensorflow.python.keras.layers import Add\nfrom tensorflow.python.keras.layers import Dropout\nfrom tensorflow.python.keras.layers import BatchNormalization\nfrom tensorflow.python.keras.layers import Conv2D\nfrom tensorflow.python.keras.layers import DepthwiseConv2D\nfrom tensorflow.python.keras.layers import ZeroPadding2D\nfrom tensorflow.python.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.python.keras.utils.layer_utils import get_source_inputs\nfrom tensorflow.python.keras.utils.data_utils import get_file\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.keras.applications.imagenet_utils import preprocess_input\n\nWEIGHTS_PATH_X = \"https://github.com/bonlime/keras-deeplab-v3-plus/releases/download/1.1/deeplabv3_xception_tf_dim_ordering_tf_kernels.h5\"\nWEIGHTS_PATH_MOBILE = \"https://github.com/bonlime/keras-deeplab-v3-plus/releases/download/1.1/deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels.h5\"\n\nWEIGHTS_PATH_X_CS = \"https://github.com/bonlime/keras-deeplab-v3-plus/releases/download/1.2/deeplabv3_xception_tf_dim_ordering_tf_kernels_cityscapes.h5\"\nWEIGHTS_PATH_MOBILE_CS = \"https://github.com/bonlime/keras-deeplab-v3-plus/releases/download/1.2/deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels_cityscapes.h5\"\n\n\ndef SepConv_BN(x, filters, prefix, stride=1, kernel_size=3, rate=1, depth_activation=False, epsilon=1e-3):\n    \"\"\" SepConv with BN between depthwise & pointwise. Optionally add activation after BN\n        Implements right \"same\" padding for even kernel sizes\n        Args:\n            x: input tensor\n            filters: num of filters in pointwise convolution\n            prefix: prefix before name\n            stride: stride at depthwise conv\n            kernel_size: kernel size for depthwise convolution\n            rate: atrous rate for depthwise convolution\n            depth_activation: flag to use activation between depthwise & poinwise convs\n            epsilon: epsilon to use in BN layer\n    \"\"\"\n\n    if stride == 1:\n        depth_padding = 'same'\n    else:\n        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n        pad_total = kernel_size_effective - 1\n        pad_beg = pad_total // 2\n        pad_end = pad_total - pad_beg\n        x = ZeroPadding2D((pad_beg, pad_end))(x)\n        depth_padding = 'valid'\n\n    if not depth_activation:\n        x = Activation(tf.nn.relu)(x)\n    x = DepthwiseConv2D((kernel_size, kernel_size), strides=(stride, stride), dilation_rate=(rate, rate),\n                        padding=depth_padding, use_bias=False, name=prefix + '_depthwise')(x)\n    x = BatchNormalization(name=prefix + '_depthwise_BN', epsilon=epsilon)(x)\n    if depth_activation:\n        x = Activation(tf.nn.relu)(x)\n    x = Conv2D(filters, (1, 1), padding='same',\n               use_bias=False, name=prefix + '_pointwise')(x)\n    x = BatchNormalization(name=prefix + '_pointwise_BN', epsilon=epsilon)(x)\n    if depth_activation:\n        x = Activation(tf.nn.relu)(x)\n\n    return x\n\n\ndef _conv2d_same(x, filters, prefix, stride=1, kernel_size=3, rate=1):\n    \"\"\"Implements right 'same' padding for even kernel sizes\n        Without this there is a 1 pixel drift when stride = 2\n        Args:\n            x: input tensor\n            filters: num of filters in pointwise convolution\n            prefix: prefix before name\n            stride: stride at depthwise conv\n            kernel_size: kernel size for depthwise convolution\n            rate: atrous rate for depthwise convolution\n    \"\"\"\n    if stride == 1:\n        return Conv2D(filters,\n                      (kernel_size, kernel_size),\n                      strides=(stride, stride),\n                      padding='same', use_bias=False,\n                      dilation_rate=(rate, rate),\n                      name=prefix)(x)\n    else:\n        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n        pad_total = kernel_size_effective - 1\n        pad_beg = pad_total // 2\n        pad_end = pad_total - pad_beg\n        x = ZeroPadding2D((pad_beg, pad_end))(x)\n        return Conv2D(filters,\n                      (kernel_size, kernel_size),\n                      strides=(stride, stride),\n                      padding='valid', use_bias=False,\n                      dilation_rate=(rate, rate),\n                      name=prefix)(x)\n\n\ndef _xception_block(inputs, depth_list, prefix, skip_connection_type, stride,\n                    rate=1, depth_activation=False, return_skip=False):\n    \"\"\" Basic building block of modified Xception network\n        Args:\n            inputs: input tensor\n            depth_list: number of filters in each SepConv layer. len(depth_list) == 3\n            prefix: prefix before name\n            skip_connection_type: one of {'conv','sum','none'}\n            stride: stride at last depthwise conv\n            rate: atrous rate for depthwise convolution\n            depth_activation: flag to use activation between depthwise & pointwise convs\n            return_skip: flag to return additional tensor after 2 SepConvs for decoder\n            \"\"\"\n    residual = inputs\n    for i in range(3):\n        residual = SepConv_BN(residual,\n                              depth_list[i],\n                              prefix + '_separable_conv{}'.format(i + 1),\n                              stride=stride if i == 2 else 1,\n                              rate=rate,\n                              depth_activation=depth_activation)\n        if i == 1:\n            skip = residual\n    if skip_connection_type == 'conv':\n        shortcut = _conv2d_same(inputs, depth_list[-1], prefix + '_shortcut',\n                                kernel_size=1,\n                                stride=stride)\n        shortcut = BatchNormalization(name=prefix + '_shortcut_BN')(shortcut)\n        outputs = layers.add([residual, shortcut])\n    elif skip_connection_type == 'sum':\n        outputs = layers.add([residual, inputs])\n    elif skip_connection_type == 'none':\n        outputs = residual\n    if return_skip:\n        return outputs, skip\n    else:\n        return outputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id, skip_connection, rate=1):\n    in_channels = inputs.shape[-1]#.value  # inputs._keras_shape[-1]\n    pointwise_conv_filters = int(filters * alpha)\n    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n    x = inputs\n    prefix = 'expanded_conv_{}_'.format(block_id)\n    if block_id:\n        # Expand\n\n        x = Conv2D(expansion * in_channels, kernel_size=1, padding='same',\n                   use_bias=False, activation=None,\n                   name=prefix + 'expand')(x)\n        x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n                               name=prefix + 'expand_BN')(x)\n        x = Activation(tf.nn.relu6, name=prefix + 'expand_relu')(x)\n    else:\n        prefix = 'expanded_conv_'\n    # Depthwise\n    x = DepthwiseConv2D(kernel_size=3, strides=stride, activation=None,\n                        use_bias=False, padding='same', dilation_rate=(rate, rate),\n                        name=prefix + 'depthwise')(x)\n    x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n                           name=prefix + 'depthwise_BN')(x)\n\n    x = Activation(tf.nn.relu6, name=prefix + 'depthwise_relu')(x)\n\n    # Project\n    x = Conv2D(pointwise_filters,\n               kernel_size=1, padding='same', use_bias=False, activation=None,\n               name=prefix + 'project')(x)\n    x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n                           name=prefix + 'project_BN')(x)\n\n    if skip_connection:\n        return Add(name=prefix + 'add')([inputs, x])\n\n    # if in_channels == pointwise_filters and stride == 1:\n    #    return Add(name='res_connect_' + str(block_id))([inputs, x])\n\n    return x\n\n\ndef Deeplabv3(weights='pascal_voc', input_tensor=None, input_shape=(512, 512, 3), classes=21, backbone='mobilenetv2',\n              OS=16, alpha=1., activation=None):\n    \"\"\" Instantiates the Deeplabv3+ architecture\n    Optionally loads weights pre-trained\n    on PASCAL VOC or Cityscapes. This model is available for TensorFlow only.\n    # Arguments\n        weights: one of 'pascal_voc' (pre-trained on pascal voc),\n            'cityscapes' (pre-trained on cityscape) or None (random initialization)\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        input_shape: shape of input image. format HxWxC\n            PASCAL VOC model was trained on (512,512,3) images. None is allowed as shape/width\n        classes: number of desired classes. PASCAL VOC has 21 classes, Cityscapes has 19 classes.\n            If number of classes not aligned with the weights used, last layer is initialized randomly\n        backbone: backbone to use. one of {'xception','mobilenetv2'}\n        activation: optional activation to add to the top of the network.\n            One of 'softmax', 'sigmoid' or None\n        OS: determines input_shape/feature_extractor_output ratio. One of {8,16}.\n            Used only for xception backbone.\n        alpha: controls the width of the MobileNetV2 network. This is known as the\n            width multiplier in the MobileNetV2 paper.\n                - If `alpha` < 1.0, proportionally decreases the number\n                    of filters in each layer.\n                - If `alpha` > 1.0, proportionally increases the number\n                    of filters in each layer.\n                - If `alpha` = 1, default number of filters from the paper\n                    are used at each layer.\n            Used only for mobilenetv2 backbone. Pretrained is only available for alpha=1.\n    # Returns\n        A Keras model instance.\n    # Raises\n        RuntimeError: If attempting to run this model with a\n            backend that does not support separable convolutions.\n        ValueError: in case of invalid argument for `weights` or `backbone`\n    \"\"\"\n\n    if not (weights in {'pascal_voc', 'cityscapes', None}):\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization), `pascal_voc`, or `cityscapes` '\n                         '(pre-trained on PASCAL VOC)')\n\n    if not (backbone in {'xception', 'mobilenetv2'}):\n        raise ValueError('The `backbone` argument should be either '\n                         '`xception`  or `mobilenetv2` ')\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        img_input = input_tensor\n\n    if backbone == 'xception':\n        if OS == 8:\n            entry_block3_stride = 1\n            middle_block_rate = 2  # ! Not mentioned in paper, but required\n            exit_block_rates = (2, 4)\n            atrous_rates = (12, 24, 36)\n        else:\n            entry_block3_stride = 2\n            middle_block_rate = 1\n            exit_block_rates = (1, 2)\n            atrous_rates = (6, 12, 18)\n\n        x = Conv2D(32, (3, 3), strides=(2, 2),\n                   name='entry_flow_conv1_1', use_bias=False, padding='same')(img_input)\n        x = BatchNormalization(name='entry_flow_conv1_1_BN')(x)\n        x = Activation(tf.nn.relu)(x)\n\n        x = _conv2d_same(x, 64, 'entry_flow_conv1_2', kernel_size=3, stride=1)\n        x = BatchNormalization(name='entry_flow_conv1_2_BN')(x)\n        x = Activation(tf.nn.relu)(x)\n\n        x = _xception_block(x, [128, 128, 128], 'entry_flow_block1',\n                            skip_connection_type='conv', stride=2,\n                            depth_activation=False)\n        x, skip1 = _xception_block(x, [256, 256, 256], 'entry_flow_block2',\n                                   skip_connection_type='conv', stride=2,\n                                   depth_activation=False, return_skip=True)\n\n        x = _xception_block(x, [728, 728, 728], 'entry_flow_block3',\n                            skip_connection_type='conv', stride=entry_block3_stride,\n                            depth_activation=False)\n        for i in range(16):\n            x = _xception_block(x, [728, 728, 728], 'middle_flow_unit_{}'.format(i + 1),\n                                skip_connection_type='sum', stride=1, rate=middle_block_rate,\n                                depth_activation=False)\n\n        x = _xception_block(x, [728, 1024, 1024], 'exit_flow_block1',\n                            skip_connection_type='conv', stride=1, rate=exit_block_rates[0],\n                            depth_activation=False)\n        x = _xception_block(x, [1536, 1536, 2048], 'exit_flow_block2',\n                            skip_connection_type='none', stride=1, rate=exit_block_rates[1],\n                            depth_activation=True)\n\n    else:\n        OS = 8\n        first_block_filters = _make_divisible(32 * alpha, 8)\n        x = Conv2D(first_block_filters,\n                   kernel_size=3,\n                   strides=(2, 2), padding='same', use_bias=False,\n                   name='Conv' if input_shape[2] == 3 else 'Conv_')(img_input)\n        x = BatchNormalization(\n            epsilon=1e-3, momentum=0.999, name='Conv_BN')(x)\n        x = Activation(tf.nn.relu6, name='Conv_Relu6')(x)\n\n        x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n                                expansion=1, block_id=0, skip_connection=False)\n\n        x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n                                expansion=6, block_id=1, skip_connection=False)\n        x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n                                expansion=6, block_id=2, skip_connection=True)\n\n        x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n                                expansion=6, block_id=3, skip_connection=False)\n        x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n                                expansion=6, block_id=4, skip_connection=True)\n        x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n                                expansion=6, block_id=5, skip_connection=True)\n\n        # stride in block 6 changed from 2 -> 1, so we need to use rate = 2\n        x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,  # 1!\n                                expansion=6, block_id=6, skip_connection=False)\n        x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, rate=2,\n                                expansion=6, block_id=7, skip_connection=True)\n        x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, rate=2,\n                                expansion=6, block_id=8, skip_connection=True)\n        x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, rate=2,\n                                expansion=6, block_id=9, skip_connection=True)\n\n        x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1, rate=2,\n                                expansion=6, block_id=10, skip_connection=False)\n        x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1, rate=2,\n                                expansion=6, block_id=11, skip_connection=True)\n        x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1, rate=2,\n                                expansion=6, block_id=12, skip_connection=True)\n\n        x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1, rate=2,  # 1!\n                                expansion=6, block_id=13, skip_connection=False)\n        x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1, rate=4,\n                                expansion=6, block_id=14, skip_connection=True)\n        x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1, rate=4,\n                                expansion=6, block_id=15, skip_connection=True)\n\n        x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1, rate=4,\n                                expansion=6, block_id=16, skip_connection=False)\n\n    # end of feature extractor\n\n    # branching for Atrous Spatial Pyramid Pooling\n\n    # Image Feature branch\n    shape_before = tf.shape(x)\n    b4 = GlobalAveragePooling2D()(x)\n    b4_shape = tf.keras.backend.int_shape(b4)\n    # from (b_size, channels)->(b_size, 1, 1, channels)\n    b4 = Reshape((1, 1, b4_shape[1]))(b4)\n    b4 = Conv2D(256, (1, 1), padding='same',\n                use_bias=False, name='image_pooling')(b4)\n    b4 = BatchNormalization(name='image_pooling_BN', epsilon=1e-5)(b4)\n    b4 = Activation(tf.nn.relu)(b4)\n    # upsample. have to use compat because of the option align_corners\n    size_before = tf.keras.backend.int_shape(x)\n    b4 = tf.keras.layers.experimental.preprocessing.Resizing(\n            *size_before[1:3], interpolation=\"bilinear\"\n        )(b4)\n    # simple 1x1\n    b0 = Conv2D(256, (1, 1), padding='same', use_bias=False, name='aspp0')(x)\n    b0 = BatchNormalization(name='aspp0_BN', epsilon=1e-5)(b0)\n    b0 = Activation(tf.nn.relu, name='aspp0_activation')(b0)\n\n    # there are only 2 branches in mobilenetV2. not sure why\n    if backbone == 'xception':\n        # rate = 6 (12)\n        b1 = SepConv_BN(x, 256, 'aspp1',\n                        rate=atrous_rates[0], depth_activation=True, epsilon=1e-5)\n        # rate = 12 (24)\n        b2 = SepConv_BN(x, 256, 'aspp2',\n                        rate=atrous_rates[1], depth_activation=True, epsilon=1e-5)\n        # rate = 18 (36)\n        b3 = SepConv_BN(x, 256, 'aspp3',\n                        rate=atrous_rates[2], depth_activation=True, epsilon=1e-5)\n\n        # concatenate ASPP branches & project\n        x = Concatenate()([b4, b0, b1, b2, b3])\n    else:\n        x = Concatenate()([b4, b0])\n\n    x = Conv2D(256, (1, 1), padding='same',\n               use_bias=False, name='concat_projection')(x)\n    x = BatchNormalization(name='concat_projection_BN', epsilon=1e-5)(x)\n    x = Activation(tf.nn.relu)(x)\n    x = Dropout(0.1)(x)\n    # DeepLab v.3+ decoder\n\n    if backbone == 'xception':\n        # Feature projection\n        # x4 (x2) block\n        skip_size = tf.keras.backend.int_shape(skip1)\n        x = tf.keras.layers.experimental.preprocessing.Resizing(\n                *skip_size[1:3], interpolation=\"bilinear\"\n            )(x)\n        dec_skip1 = Conv2D(48, (1, 1), padding='same',\n                           use_bias=False, name='feature_projection0')(skip1)\n        dec_skip1 = BatchNormalization(\n            name='feature_projection0_BN', epsilon=1e-5)(dec_skip1)\n        dec_skip1 = Activation(tf.nn.relu)(dec_skip1)\n        x = Concatenate()([x, dec_skip1])\n        x = SepConv_BN(x, 256, 'decoder_conv0',\n                       depth_activation=True, epsilon=1e-5)\n        x = SepConv_BN(x, 256, 'decoder_conv1',\n                       depth_activation=True, epsilon=1e-5)\n\n    # you can use it with arbitary number of classes\n    if (weights == 'pascal_voc' and classes == 21) or (weights == 'cityscapes' and classes == 19):\n        last_layer_name = 'logits_semantic'\n    else:\n        last_layer_name = 'custom_logits_semantic'\n\n    x = Conv2D(classes, (1, 1), padding='same', name=last_layer_name)(x)\n    size_before3 = tf.keras.backend.int_shape(img_input)\n    x = tf.keras.layers.experimental.preprocessing.Resizing(\n            *size_before3[1:3], interpolation=\"bilinear\"\n        )(x)\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n\n    if activation in {'softmax', 'sigmoid'}:\n        x = tf.keras.layers.Activation(activation)(x)\n\n    model = Model(inputs, x, name='deeplabv3plus')\n\n    # load weights\n\n    if weights == 'pascal_voc':\n        if backbone == 'xception':\n            weights_path = get_file('deeplabv3_xception_tf_dim_ordering_tf_kernels.h5',\n                                    WEIGHTS_PATH_X,\n                                    cache_subdir='models')\n        else:\n            weights_path = get_file('deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels.h5',\n                                    WEIGHTS_PATH_MOBILE,\n                                    cache_subdir='models')\n        model.load_weights(weights_path, by_name=True)\n    elif weights == 'cityscapes':\n        if backbone == 'xception':\n            weights_path = get_file('deeplabv3_xception_tf_dim_ordering_tf_kernels_cityscapes.h5',\n                                    WEIGHTS_PATH_X_CS,\n                                    cache_subdir='models')\n#             weights_path = \"pretrained_weights/deeplabv3_xception_tf_dim_ordering_tf_kernels_cityscapes.h5\"    \n        else:\n            weights_path = get_file('deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels_cityscapes.h5',\n                                    WEIGHTS_PATH_MOBILE_CS,\n                                    cache_subdir='models')\n#            weights_path = \"pretrained_weights/deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels_cityscapes.h5\"\n            \n        model.load_weights(weights_path, by_name=True)\n    return model\n\ndef preprocess_input(x):\n    \"\"\"Preprocesses a numpy array encoding a batch of images.\n    # Arguments\n        x: a 4D numpy array consists of RGB values within [0, 255].\n    # Returns\n        Input array scaled to [-1.,1.]\n    \"\"\"\n    return preprocess_input(x, mode='tf')","metadata":{"execution":{"iopub.status.busy":"2022-02-23T03:40:38.498367Z","iopub.execute_input":"2022-02-23T03:40:38.498623Z","iopub.status.idle":"2022-02-23T03:40:38.580619Z","shell.execute_reply.started":"2022-02-23T03:40:38.49859Z","shell.execute_reply":"2022-02-23T03:40:38.579723Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = Deeplabv3(weights='cityscapes', input_shape=(*IMG_SIZE,3), classes=1, \n#                   backbone= 'mobilenetv2', #'xception'\n#                   OS=16, alpha=1, activation='sigmoid')\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T03:40:38.584074Z","iopub.execute_input":"2022-02-23T03:40:38.584842Z","iopub.status.idle":"2022-02-23T03:40:39.918903Z","shell.execute_reply.started":"2022-02-23T03:40:38.584805Z","shell.execute_reply":"2022-02-23T03:40:39.918193Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DeeplabV3p custom","metadata":{}},{"cell_type":"code","source":"def convolution_block(\n    block_input,\n    num_filters=256,\n    kernel_size=3,\n    dilation_rate=1,\n    padding=\"same\",\n    use_bias=False,\n):\n    x = layers.SeparableConv2D(\n        num_filters,\n        kernel_size=kernel_size,\n        dilation_rate=dilation_rate,\n        padding=\"same\",\n        use_bias=use_bias,\n        kernel_initializer=keras.initializers.HeNormal(),\n    )(block_input)\n    x = layers.BatchNormalization()(x)\n    return tf.nn.relu(x)\n\n\ndef DilatedSpatialPyramidPooling(dspp_input):\n    dims = dspp_input.shape\n    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n    x = convolution_block(x, kernel_size=1, use_bias=True)\n    out_pool = layers.UpSampling2D(\n        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n    )(x)\n\n    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n\n    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n    output = convolution_block(x, kernel_size=1)\n    return output","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:00:47.026325Z","iopub.execute_input":"2022-02-23T08:00:47.02662Z","iopub.status.idle":"2022-02-23T08:00:47.038886Z","shell.execute_reply.started":"2022-02-23T08:00:47.026579Z","shell.execute_reply":"2022-02-23T08:00:47.037726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization\nfrom tensorflow.keras.layers import UpSampling2D, Input, Concatenate,SeparableConv2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import MobileNetV2\ndef DeeplabV3Plus(image_size, num_classes):\n    inputs = keras.Input(shape=(image_size[0], image_size[1], 3),name=\"input_image\")\n    encoder = MobileNetV2(\n        input_tensor=inputs,\n        weights=\"imagenet\",\n        include_top=False, alpha=0.35)\n    skip_connection_names = [\"input_image\", \"block_3_expand_relu\"]\n    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output    \n    x = DilatedSpatialPyramidPooling(encoder_output)\n\n    f = [16, 32]\n    for i in range(1, len(skip_connection_names)+1, 1):                \n        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n        x = Conv2D(f[-i], (1, 1))(x) \n        x = UpSampling2D((4, 4),interpolation=\"bilinear\")(x)\n        print(x.shape)\n        x = Concatenate()([x, x_skip])        \n        x = SeparableConv2D(f[-i], (3, 3), padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        \n#         x = SeparableConv2D(f[-i], (3, 3), padding=\"same\")(x)\n#         x = BatchNormalization()(x)\n#         x = Activation(\"relu\")(x)        \n    \n    x = Conv2D(num_classes, (1, 1), padding=\"same\")(x)\n    x = Activation(\"sigmoid\")(x)\n    \n    model = Model(inputs, x)\n    \n    return model\n\n\nmodel = DeeplabV3Plus(image_size=IMG_SIZE, num_classes=1)\nmodel.summary()\n\n# model = Deeplabv3(weights='cityscapes', input_shape=(*IMG_SIZE,3), classes=1, \n#                   backbone= 'mobilenetv2', #'xception'\n#                   OS=16, alpha=1, activation='sigmoid')\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:00:47.448674Z","iopub.execute_input":"2022-02-23T08:00:47.449195Z","iopub.status.idle":"2022-02-23T08:00:48.648162Z","shell.execute_reply.started":"2022-02-23T08:00:47.449159Z","shell.execute_reply":"2022-02-23T08:00:48.647397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model training","metadata":{}},{"cell_type":"code","source":"!pip install segmentation_models","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:58:18.137987Z","iopub.execute_input":"2022-02-23T07:58:18.138249Z","iopub.status.idle":"2022-02-23T07:58:28.314529Z","shell.execute_reply.started":"2022-02-23T07:58:18.138218Z","shell.execute_reply":"2022-02-23T07:58:28.313636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from segmentation_models.losses import bce_jaccard_loss, dice_loss, JaccardLoss\nfrom segmentation_models.metrics import iou_score, f1_score, precision, recall\nls = dice_loss + bce_jaccard_loss\nmetrics = [precision, recall, f1_score, iou_score] ","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:01:01.459389Z","iopub.execute_input":"2022-02-23T08:01:01.45966Z","iopub.status.idle":"2022-02-23T08:01:01.725259Z","shell.execute_reply.started":"2022-02-23T08:01:01.459629Z","shell.execute_reply":"2022-02-23T08:01:01.722795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os, time, keras\n# %env SM_FRAMEWORK=tf.keras\n\n# import numpy as np\n# import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:01:02.464882Z","iopub.execute_input":"2022-02-23T08:01:02.465521Z","iopub.status.idle":"2022-02-23T08:01:02.46995Z","shell.execute_reply.started":"2022-02-23T08:01:02.465483Z","shell.execute_reply":"2022-02-23T08:01:02.468408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, time, keras\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n\nbase_dir = 'RESULTS/Dv3p_MobilenetV2_alpha0_35_custom'\nif not os.path.exists(\"RESULTS\"):\n    os.mkdir(\"RESULTS/\")\nif not os.path.exists(base_dir):\n    os.mkdir(base_dir)\n    os.mkdir(f\"{base_dir}/ckpt_path\")\n    \ncsv_path = f\"{base_dir}/history.csv\"\n\n\"\"\" callbacks \"\"\"\nroot_logdir = os.path.join(os.curdir, f\"{base_dir}/logs\",\"fit\",\"\")\ndef get_run_logdir():\n    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n    return os.path.join(root_logdir, run_id)\nrun_logdir = get_run_logdir()\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)#, histogram_freq=1,profile_batch='10,15')\n\ncheckpoint_filepath = f'{base_dir}/'+'ckpt_path/epoch.h5'\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    # monitor='val_iou_score',\n    # mode='max',\n    verbose = 1,\n    period = 1,\n    save_best_only=False\n    )\n\ncallbacks = [\n    model_checkpoint_callback,\n    ReduceLROnPlateau(monitor=\"val_loss\", patience=5, factor=0.1, verbose=1),\n    CSVLogger(csv_path),\n#     EarlyStopping(monitor=\"val_loss\", patience=10),\n    tensorboard_cb\n]\n\n\"\"\" steps per epochs \"\"\"\ntrain_steps = len(train_x)//BATCH_SIZE\nif len(train_x) % BATCH_SIZE != 0:\n    train_steps += 1\n\ntest_steps = len(val_x)//BATCH_SIZE\nif len(val_x) % BATCH_SIZE != 0:\n    test_steps += 1\n\nprint(\"train_steps\", train_steps, \"test_steps\",test_steps)\n\n# \"\"\" Model training \"\"\"\n# for layer in model.layers:\n#     if layer.name == \"global_average_pooling2d\":\n#         break\n#     else:\n#         layer.trainable = False\n\n# for layer in model.layers:\n#     print(layer.name,layer.trainable)\n\nmodel.compile(\n    loss=ls,\n    optimizer= \"adam\", #tf.keras.optimizers.Adam(lr),\n    metrics=metrics\n)\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:02:10.043996Z","iopub.execute_input":"2022-02-23T08:02:10.044252Z","iopub.status.idle":"2022-02-23T08:02:10.335551Z","shell.execute_reply.started":"2022-02-23T08:02:10.044222Z","shell.execute_reply":"2022-02-23T08:02:10.334852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # !pip install tensorboard\n# %load_ext tensorboard\n# %tensorboard --logdir ./RESULTS/MobilenetV2_alpha1/logs","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:02:13.059315Z","iopub.execute_input":"2022-02-23T08:02:13.059889Z","iopub.status.idle":"2022-02-23T08:02:13.063894Z","shell.execute_reply.started":"2022-02-23T08:02:13.059852Z","shell.execute_reply":"2022-02-23T08:02:13.062495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pretrain model decoder\nhistory = model.fit(\n    dataset[\"train\"],\n    validation_data=dataset[\"val\"],\n    epochs=10,\n    initial_epoch = 0,\n    steps_per_epoch=train_steps,\n    validation_steps=test_steps,\n    callbacks=callbacks\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:02:17.797118Z","iopub.execute_input":"2022-02-23T08:02:17.797393Z","iopub.status.idle":"2022-02-23T08:33:23.587152Z","shell.execute_reply.started":"2022-02-23T08:02:17.797362Z","shell.execute_reply":"2022-02-23T08:33:23.586413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # pretrain model decoder\n# history = model.fit(\n#     dataset[\"train\"],\n#     validation_data=dataset[\"val\"],\n#     epochs= 25,\n#     initial_epoch = 10,\n#     steps_per_epoch=train_steps,\n#     validation_steps=test_steps,\n#     callbacks=callbacks\n# )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DATA Visualization\nhttps://www.kaggle.com/vfdev5/data-visualization","metadata":{}},{"cell_type":"markdown","source":"## Augmentation methods\nhttps://www.kaggle.com/gaborfodor/augmentation-methods\n\nContrast is the difference between light and dark.\nSaturation is the intensity of color.\nBrightness refers to the relative lightness or darkness of a color.\n\nrandom_channel_shift(img, limit=0.05)\nrandom_brightness(img, limit=(-0.5, 0.5), u=0.5)\nrandom_contrast(img, limit=(-0.5, 0.5), u=0.5)\nrandom_saturation(img, limit=(-0.5, 0.5), u=0.5)\nrandom_gray(img, u=0.2)\nrandom_rotate(img, mask, rotate_limit=(-20, 20), u=0.5)\nrandom_shear(img, mask, intensity_range=(-0.3, 0.3), u=0.2)\nrandom_flip(img, mask, u=0.3)\nrandom_shift(img, mask, w_limit=(-0.1, 0.1), h_limit=(-0.1, 0.1), u=0.3)\nrandom_zoom(img, mask, zoom_range=(0.8, 1), u=0.3)","metadata":{}},{"cell_type":"markdown","source":"## Predictions on Validation set","metadata":{}},{"cell_type":"code","source":"req_cmap = {\n        0: (0,0,0), # background                        \n        1: (255,0,0),    # car \n        2: (255,255,255)    # miou label\n        }\nreq_mask_labels = {\n    0:\"Background\",    \n    1:\"Car\",\n    2:\"miou\"\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:34:25.483842Z","iopub.execute_input":"2022-02-23T08:34:25.48411Z","iopub.status.idle":"2022-02-23T08:34:25.491363Z","shell.execute_reply.started":"2022-02-23T08:34:25.484081Z","shell.execute_reply":"2022-02-23T08:34:25.48901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef convert_to_rgb(predi,_cmap={}):\n    pred_image = np.zeros((predi.shape[0], predi.shape[1], 3),dtype=np.uint8) + 255\n    for i in np.unique(predi):\n        pred_image[predi==i] = _cmap[i]\n    return pred_image\n\ndef plot_imgs(i,img,mask,pred=np.zeros((1024,1024,3)),cmap={},mask_labels={},label_iou={}):  \n    fig,(ax1,ax2,ax3, ax4) = plt.subplots(1,4,figsize=(20,4))\n    if img.shape[-1]==3:\n        ax1.imshow(img)\n    else:\n        ax1.imshow(img,cmap=plt.get_cmap('gray'))\n    ax1.axis('off')\n    ax1.title.set_text(f\"Input Image {i}\") \n\n    ax2.imshow(mask)\n    ax2.axis('off')\n    ax2.title.set_text(f\"Ground truth {i}\")\n\n    ax3.imshow(pred)\n    ax3.axis('off')   \n    ax3.title.set_text(f\"Prediction {i}\")  \n\n    dst = cv2.addWeighted(np.asarray(img*255.0,dtype=np.uint8),1,pred,0.5,0)\n    ax4.imshow(dst)\n    ax4.axis('off')\n    ax4.title.set_text(f\"Overlay {i}\") \n\n    patches = [ mpatches.Patch(color=np.array(cmap[i])/255.0, label=\"{:<15}:{:2.3f} \".format(mask_labels[i],label_iou[i])) for i in label_iou.keys()]\n    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n\n#     path = \"xception/output_result\"\n#     if not os.path.exists(path):\n#         os.mkdir(path)\n    img_name = f\"{i}.png\"\n#     fig.set_size_inches(25, 10)\n#     plt.savefig(f'{path}/{img_name}', dpi=100)\n    plt.show()\n    plt.close(fig)\n\n    \ndef IoU(Yi,y_predi,mask_labels={}):\n    \n#     y_true_f = K.flatten(Yi)\n#     y_pred_f = K.flatten(y_predi)\n#     intersection = K.sum(y_true_f * y_pred_f)\n#     smooth = 0.001\n#     dice_loss = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n#     return [dice_loss]\n    \n  ## mean Intersection over Union\n  ## Mean IoU = TP/(FN + TP + FP)\n  \n    IoUs = [] \n    precisions = []\n    recalls = []\n    f1_scores=[]\n    f2_scores=[]\n    #   dice_scores = []\n\n    labels_iou = {}  \n    for c in mask_labels.keys():      \n        TP = np.sum( (Yi == c)&(y_predi==c) )\n        FP = np.sum( (Yi != c)&(y_predi==c) )\n        FN = np.sum( (Yi == c)&(y_predi != c)) \n        TN = np.sum( (Yi != c)&(y_predi != c)) \n\n        IoU = TP/float(TP + FP + FN)\n        precision = TP/float(TP + FP)\n        recall = TP/float(TP + FN)\n\n        beta= 1\n        f1_score = ((1+beta**2)*precision*recall)/float(beta**2*precision + recall)\n\n        beta= 2\n        f2_score  = ((1+beta**2)*precision*recall)/float(beta**2*precision + recall)\n\n        #     dice_score = (2*TP)/float(2*TP + FP + FN)\n\n        if IoU > 0:\n#             print(\"class {:2.0f} {:10}:\\t TP= {:6.0f},\\t FP= {:6.0f},\\t FN= {:6.0f},\\t TN= {:6.0f},\\t IoU= {:6.3f}\".format(c,mask_labels[c],TP,FP,FN,TN,IoU))                    \n\n            labels_iou[c] = IoU\n            IoUs.append(IoU)  \n            precisions.append(precision) \n            recalls.append(recall)  \n            f1_scores.append(f1_score)  \n            f2_scores.append(f2_score) \n            #       dice_scores.append(dice_score) \n\n    mIoU = np.mean(IoUs)\n    labels_iou[len(req_mask_labels)-1] = mIoU\n#     print(\"Mean IoU: {:4.6f}\".format(mIoU))  \n\n    return labels_iou, [mIoU, np.mean(precisions),np.mean(recalls),np.mean(f1_scores), np.mean(f2_scores)]\n","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:56:34.102642Z","iopub.execute_input":"2022-02-23T08:56:34.103088Z","iopub.status.idle":"2022-02-23T08:56:34.122606Z","shell.execute_reply.started":"2022-02-23T08:56:34.103045Z","shell.execute_reply":"2022-02-23T08:56:34.121635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport matplotlib.patches as mpatches\n# df = pd.DataFrame(columns=columns)\nval_data = iter(dataset[\"val\"])\n\nfor j in tqdm(range(0,len(val_X_y_paths),BATCH_SIZE)):\n    img, mask = next(val_data)                         \n    y_pred = model.predict(img) \n    \n    for i in range(BATCH_SIZE):\n        maski = np.squeeze(mask[i], axis=-1)\n        y_pred_converted = np.where(y_pred[i] > 0.5, 1, 0)\n        y_predi = tf.image.resize(y_pred_converted,IMG_SIZE ,method='nearest')\n        y_predi = np.squeeze(y_predi, axis=-1)        \n        label_iou, eval_=IoU(maski, y_predi, mask_labels=req_mask_labels)\n        \n        input_img = tf.image.resize(img[i],IMG_SIZE ,method='nearest') \n        ground_truth = np.squeeze(convert_to_rgb(maski, req_cmap))    \n        prediction = np.squeeze(convert_to_rgb(y_predi, req_cmap))\n        \n        plot_imgs(i+j+1,input_img, ground_truth, prediction,cmap=req_cmap,mask_labels=req_mask_labels,label_iou=label_iou)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:34:34.281021Z","iopub.execute_input":"2022-02-23T08:34:34.281269Z","iopub.status.idle":"2022-02-23T08:34:38.400957Z","shell.execute_reply.started":"2022-02-23T08:34:34.281241Z","shell.execute_reply":"2022-02-23T08:34:38.40027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEST RESULT","metadata":{}},{"cell_type":"code","source":"!rm -r train/","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:35:43.02476Z","iopub.execute_input":"2022-02-23T08:35:43.025179Z","iopub.status.idle":"2022-02-23T08:35:44.121012Z","shell.execute_reply.started":"2022-02-23T08:35:43.025139Z","shell.execute_reply":"2022-02-23T08:35:44.119922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! unzip -q ../input/carvana-image-masking-challenge/test.zip -d ./","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:57:13.45149Z","iopub.execute_input":"2022-02-23T08:57:13.452253Z","iopub.status.idle":"2022-02-23T08:59:44.611454Z","shell.execute_reply.started":"2022-02-23T08:57:13.452215Z","shell.execute_reply":"2022-02-23T08:59:44.610418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:59:44.616098Z","iopub.execute_input":"2022-02-23T08:59:44.616351Z","iopub.status.idle":"2022-02-23T08:59:44.826376Z","shell.execute_reply.started":"2022-02-23T08:59:44.616321Z","shell.execute_reply":"2022-02-23T08:59:44.825625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q segmentation-models","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:59:44.827729Z","iopub.execute_input":"2022-02-23T08:59:44.828206Z","iopub.status.idle":"2022-02-23T08:59:54.398101Z","shell.execute_reply.started":"2022-02-23T08:59:44.828148Z","shell.execute_reply":"2022-02-23T08:59:54.397231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom tensorflow import keras\nimport tensorflow as tf\n\nfrom segmentation_models.losses import bce_jaccard_loss, dice_loss, JaccardLoss\nfrom segmentation_models.metrics import iou_score, f1_score, precision, recall\nls = dice_loss + bce_jaccard_loss\nmetrics = [precision, recall, f1_score, iou_score] \n\nmodel = keras.models.load_model('./RESULTS/Dv3p_MobilenetV2_alpha0_35_custom/ckpt_path/epoch.h5',\n                               custom_objects={\"dice_loss_plus_binary_crossentropy_plus_jaccard_loss\": ls,\n                                              \"precision\":precision,\n                                               \"recall\":recall,\n                                               \"f1-score\":f1_score,\n                                               \"iou_score\":iou_score\n                                              }\n                               )","metadata":{"execution":{"iopub.status.busy":"2022-02-23T08:59:54.402331Z","iopub.execute_input":"2022-02-23T08:59:54.402673Z","iopub.status.idle":"2022-02-23T08:59:59.544704Z","shell.execute_reply.started":"2022-02-23T08:59:54.402631Z","shell.execute_reply":"2022-02-23T08:59:59.54385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_WIDTH, IMG_HEIGHT = 320 , 480\n\nAUTO = tf.data.experimental.AUTOTUNE\nimport pandas as pd\nimport numpy as np\ndef process_img(imagePath):\n#     print(\"process_img acivated...\")\n    img = tf.io.read_file(imagePath)\n    #color images\n    img = tf.image.decode_jpeg(img, channels=3) \n    #convert unit8 tensor to floats in the [0,1]range\n    img = tf.image.convert_image_dtype(img, tf.float32) \n    #resize \n    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n\ndef resize_img2org(img):\n    return tf.image.resize(255.*img, (1280,1918))\n\ndef one_hot(img):    \n    return np.where(img>127,1,0).astype(np.uint8)\n\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n#     print(np.squeeze(img).shape,np.unique(img))\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n        \n    runs[1::2] -= runs[:-1:2]\n    return ' '.join(str(x) for x in runs)\n\nfilelist_ds = tf.data.Dataset.list_files('./test/*',shuffle=False)\nds_size= filelist_ds.cardinality().numpy()\nprint(\"Number of files in the dataset: \", ds_size)\n\nprint(\"3 samples:\")\nfor a in filelist_ds.take(3):\n    fname= a.numpy().decode(\"utf-8\")\n    print(fname)\n#     display(PIL.Image.open(fname))\n    \nBATCH_SIZE = 25\n    \nfilelist = filelist_ds.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\nds_test_batched=filelist_ds.map(process_img).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n\nprint(\"Number of batches in test: \", ds_test_batched.cardinality().numpy())\n","metadata":{"execution":{"iopub.status.busy":"2022-02-23T09:00:52.798514Z","iopub.execute_input":"2022-02-23T09:00:52.799248Z","iopub.status.idle":"2022-02-23T09:00:53.345131Z","shell.execute_reply.started":"2022-02-23T09:00:52.799212Z","shell.execute_reply":"2022-02-23T09:00:53.344197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'img': [], 'rle_mask': []})\ndf.to_csv('submission.csv', mode='w', index=False, header=True)\n\nfl = iter(filelist)\nds = iter(ds_test_batched)\n\nfor a in tqdm(range(ds_test_batched.cardinality().numpy())):\n    files = next(fl)\n    fnames= list(map(lambda x:x.numpy().decode(\"utf-8\"),files))\n    \n    data = next(ds)\n    pred_mask = model.predict(data)\n    \n    rle = list(map(rle_encode,map(one_hot,map(resize_img2org,pred_mask))))       \n    test_paths__ = list(map(lambda x: x.split(\"/\")[-1],fnames))\n    df = pd.DataFrame({'img': test_paths__, 'rle_mask': rle})\n    df.to_csv('submission.csv', mode='a', index=False, header=False)\n    \n#     break","metadata":{"execution":{"iopub.status.busy":"2022-02-23T09:00:58.596922Z","iopub.execute_input":"2022-02-23T09:00:58.597801Z","iopub.status.idle":"2022-02-23T10:00:52.453371Z","shell.execute_reply.started":"2022-02-23T09:00:58.597742Z","shell.execute_reply":"2022-02-23T10:00:52.451851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2=pd.read_csv('./submission.csv')\nlen(df2), len(np.unique(df2['img'])),df2.tail()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T10:04:36.987471Z","iopub.execute_input":"2022-02-23T10:04:36.988125Z","iopub.status.idle":"2022-02-23T10:04:46.319363Z","shell.execute_reply.started":"2022-02-23T10:04:36.988080Z","shell.execute_reply":"2022-02-23T10:04:46.318604Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(100064,\n 100064,\n                         img                                           rle_mask\n 100059  fff341f26a19_12.jpg  377721 16 379624 33 381540 36 383457 37 385372...\n 100060  fff341f26a19_13.jpg  379626 30 381542 33 383458 36 385373 17 385404...\n 100061  fff341f26a19_14.jpg  379626 30 381541 33 383457 35 385372 23 387287...\n 100062  fff341f26a19_15.jpg  377710 27 377756 2 379623 35 379672 4 381539 3...\n 100063  fff341f26a19_16.jpg  377709 28 379623 34 381539 37 383455 39 385371...)"},"metadata":{}}]},{"cell_type":"code","source":"# df2=pd.read_csv('./submission.csv.gz')\n\n# df2 = df2.drop_duplicates()\n\n# !rm ./submission.csv.gz\n\n# df2.to_csv(\"submission.csv.gz\",compression='gzip',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:19:24.14089Z","iopub.execute_input":"2022-02-23T07:19:24.141191Z","iopub.status.idle":"2022-02-23T07:22:19.697731Z","shell.execute_reply.started":"2022-02-23T07:19:24.141157Z","shell.execute_reply":"2022-02-23T07:22:19.696876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gzip submission.csv","metadata":{"execution":{"iopub.status.busy":"2022-02-23T10:05:14.059404Z","iopub.execute_input":"2022-02-23T10:05:14.060060Z","iopub.status.idle":"2022-02-23T10:06:47.250977Z","shell.execute_reply.started":"2022-02-23T10:05:14.060016Z","shell.execute_reply":"2022-02-23T10:06:47.249945Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"!rm -r test/\n# !rm submission.csv","metadata":{"execution":{"iopub.status.busy":"2022-02-23T10:07:02.051345Z","iopub.execute_input":"2022-02-23T10:07:02.052115Z","iopub.status.idle":"2022-02-23T10:07:05.145888Z","shell.execute_reply.started":"2022-02-23T10:07:02.052070Z","shell.execute_reply":"2022-02-23T10:07:05.144842Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}